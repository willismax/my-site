"use strict";(self.webpackChunkmy_site=self.webpackChunkmy_site||[]).push([[3922],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>u});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},c=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},_="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),_=p(t),d=r,u=_["".concat(s,".").concat(d)]||_[d]||m[d]||o;return t?a.createElement(u,i(i({ref:n},c),{},{components:t})):a.createElement(u,i({ref:n},c))}));function u(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[_]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=t[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},4512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=t(7462),r=(t(7294),t(3905));const o={jupyter:{jupytext:{cell_metadata_filter:"tags,-all",text_representation:{extension:".md",format_name:"markdown",format_version:"1.3",jupytext_version:"1.15.1"}},kernelspec:{display_name:"Python 3",language:"python",name:"python3"}}},i=void 0,l={unversionedId:"2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b",id:"2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b",title:"Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b",description:"- \u6b64\u7bc4\u4f8b\u6e90\u81ea snorkel-tutorials\uff0c\u76ee\u7684\u70ba\u5c0d\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2c (VRD) \u6578\u64da\u96c6\u9032\u884c\u64cd\u4f5c\uff0c\u5c08\u6ce8\u65bc\u5716\u7247\u5167\u7269\u4ef6\u4e4b\u9593\u7684\u95dc\u4fc2\u5206\u985e\u4efb\u52d9\u3002",source:"@site/docs/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/13.Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b.md",sourceDirName:"2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks",slug:"/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b",permalink:"/my-site/docs/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b",draft:!1,editUrl:"https://github.com/willismax/my-site/blob/main/docs/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/13.Snorkel_\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2cVRD\u7bc4\u4f8b.md",tags:[],version:"current",sidebarPosition:13,frontMatter:{jupyter:{jupytext:{cell_metadata_filter:"tags,-all",text_representation:{extension:".md",format_name:"markdown",format_version:"1.3",jupytext_version:"1.15.1"}},kernelspec:{display_name:"Python 3",language:"python",name:"python3"}}},sidebar:"tutorialSidebar",previous:{title:"\u5165\u9580_Snorkel",permalink:"/my-site/docs/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/\u5165\u9580_Snorkel"},next:{title:"\u7279\u5fb5\u9078\u64c7_Deta_Selection_\u9435\u4eba\u8cfd\u793a\u7bc4_",permalink:"/my-site/docs/2021ITHome\u9435\u4eba\u8cfd\u300c\u5f9eAI\u843d\u5730\u8ac7MLOps\u300d/\u5be6\u4f5cNotebooks/\u7279\u5fb5\u9078\u64c7_Deta_Selection_\u9435\u4eba\u8cfd\u793a\u7bc4_"}},s={},p=[{value:"\u8a2d\u5b9a\u5c08\u6848\u74b0\u5883",id:"\u8a2d\u5b9a\u5c08\u6848\u74b0\u5883",level:2},{value:"1. \u52a0\u8f09\u6578\u64da\u96c6",id:"1-\u52a0\u8f09\u6578\u64da\u96c6",level:2},{value:"2. \u7de8\u5bebLabeling Functions (LFs)",id:"2-\u7de8\u5beblabeling-functions-lfs",level:2},{value:"3. \u8a13\u7df4\u6a19\u7c64\u6a21\u578b",id:"3-\u8a13\u7df4\u6a19\u7c64\u6a21\u578b",level:2},{value:"4. \u8a13\u7df4\u5206\u985e\u5668",id:"4-\u8a13\u7df4\u5206\u985e\u5668",level:2},{value:"Create DataLoaders for Classifier",id:"create-dataloaders-for-classifier",level:4},{value:"\u5b9a\u7fa9\u6a21\u578b\u67b6\u69cb",id:"\u5b9a\u7fa9\u6a21\u578b\u67b6\u69cb",level:4},{value:"\u8a13\u7df4\u8207\u8a55\u4f30\u6a21\u578b",id:"\u8a13\u7df4\u8207\u8a55\u4f30\u6a21\u578b",level:3}],c={toc:p},_="wrapper";function m(e){let{components:n,...t}=e;return(0,r.kt)(_,(0,a.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("a",{href:"https://colab.research.google.com/github/willismax/ML-in-Production-30-days-sharing/blob/main/notebook/13.Snorkel_%E8%A6%96%E8%A6%BA%E9%97%9C%E4%BF%82%E6%AA%A2%E6%B8%ACVRD%E7%AF%84%E4%BE%8B.ipynb",target:"_parent"},(0,r.kt)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),(0,r.kt)("h1",{id:"13visual-relationship-detection-\u8996\u89ba\u95dc\u4fc2\u5075\u6e2c"},"13.Visual Relationship Detection \u8996\u89ba\u95dc\u4fc2\u5075\u6e2c"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u6b64\u7bc4\u4f8b\u6e90\u81ea ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/snorkel-team/snorkel-tutorials/blob/master/visual_relation/visual_relation_tutorial.ipynb"},"snorkel-tutorials"),"\uff0c\u76ee\u7684\u70ba\u5c0d\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2c (VRD) \u6578\u64da\u96c6\u9032\u884c\u64cd\u4f5c\uff0c\u5c08\u6ce8\u65bc\u5716\u7247\u5167\u7269\u4ef6\u4e4b\u9593\u7684\u95dc\u4fc2\u5206\u985e\u4efb\u52d9\u3002"),(0,r.kt)("li",{parentName:"ul"},"\u901a\u5e38\u5716\u7247\u5167\u5bb9\u7269\u90fd\u6709\u7269\u9ad4\u4e4b\u9593\u7684\u95dc\u806f\u6027\uff0c\u5b9a\u7fa9\u63cf\u8ff0\u70ba\u70ba",(0,r.kt)("inlineCode",{parentName:"li"},"a subject <predictate> object"),"\u3002",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"\u4f8b\u5982\uff0c",(0,r.kt)("inlineCode",{parentName:"li"},"person riding bicycle"),"\uff0c\u201cperson\u201d\u548c\u201cbicycle\u201d\u5206\u5225\u662f\u4e3b\u8a5e\u548c\u53d7\u8a5e\uff0c\u201criding\u201d\u662f\u95dc\u4fc2\u52d5\u8a5e\u3002"))),(0,r.kt)("li",{parentName:"ul"},"\u4ee5\u4e0b\u5716\u793a\u7d05\u8272\u6846\u4ee3\u8868\u4e3b\u984c\uff0c\u800c\u7da0\u8272\u6846\u4ee3\u8868\u5c0d\u8c61\u3002\u8a72\u4e3b\u8a5e\uff08\u5982\u8e22\uff09\u8868\u793a\u4ec0\u9ebc\u95dc\u4fc2\u9023\u63a5\u4e3b\u9ad4\u548c\u5ba2\u9ad4\u3002")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/SMxT2C4.png",alt:null})),(0,r.kt)("h2",{id:"\u8a2d\u5b9a\u5c08\u6848\u74b0\u5883"},"\u8a2d\u5b9a\u5c08\u6848\u74b0\u5883"),(0,r.kt)("p",null,"\u8907\u88fd\u5c08\u6848"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"!git clone https://github.com/snorkel-team/snorkel-tutorials.git > clone_log.txt\n!pip3 install snorkel\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"%cd snorkel-tutorials/visual_relation\n")),(0,r.kt)("p",null,"*\u6539\u5beb\u5c08\u6848\u6a94\u6848"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u56e0\u7bc4\u4f8b\u63a1\u7528\u820a\u7684pandas\uff0c\u5df2\u68c4\u7528\u7684",(0,r.kt)("inlineCode",{parentName:"li"},"df.as_matrix()"),"\u9084\u5728\u7bc4\u4f8b\u7a0b\u5f0f",(0,r.kt)("inlineCode",{parentName:"li"},"snorkel-tutorials/visual_relation/model.py"),"\u4e2d\uff0c\u70ba\u4e86\u7b26\u5408\u73fe\u4eca\u74b0\u5883\uff0c\u5c07\u6539\u5beb\u70ba",(0,r.kt)("inlineCode",{parentName:"li"},"df.values")," (model.py\u7b2c135\u884c\u9644\u8fd1)\u3002"),(0,r.kt)("li",{parentName:"ul"},"\u60a8\u53ef\u4ee5\u76f4\u63a5\u57f7\u884c\u4ee5\u4e0b",(0,r.kt)("inlineCode",{parentName:"li"},"%%writefile"),"\u6307\u4ee4\uff0c\u6216\u81ea\u884c\u5faa\u8def\u5f91\u4fee\u6b63\u6a94\u6848\u3002")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'%%writefile /content/snorkel-tutorials/visual_relation/model.py\n\n# fix _get_wordvec() df.as_matrix() to df.values (line:137)\nimport csv\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom snorkel.analysis import Scorer\nfrom snorkel.classification import DictDataset, MultitaskClassifier, Operation, Task\nfrom snorkel.classification.data import XDict, YDict\n\n\ndef union(bbox1, bbox2):\n    """Create the union of the two bboxes.\n\n    Parameters\n    ----------\n    bbox1\n        Coordinates of first bounding box\n    bbox2\n        Coordinates of second bounding box\n\n    Returns\n    -------\n    [y0, y1, x0, x1]\n        Coordinates of union of input bounding boxes\n\n    """\n    y0 = min(bbox1[0], bbox2[0])\n    y1 = max(bbox1[1], bbox2[1])\n    x0 = min(bbox1[2], bbox2[2])\n    x1 = max(bbox1[3], bbox2[3])\n    return [y0, y1, x0, x1]\n\n\ndef crop_img_arr(img_arr, bbox):\n    """Crop bounding box from image.\n\n    Parameters\n    ----------\n    img_arr\n        Image in array format\n    bbox\n        Coordinates of bounding box to crop\n\n    Returns\n    -------\n    img_arr\n        Cropped image\n\n    """\n    return img_arr[bbox[0] : bbox[1], bbox[2] : bbox[3], :]\n\n\nclass SceneGraphDataset(DictDataset):\n    """Dataloader for Scene Graph Dataset."""\n\n    def __init__(\n        self,\n        name: str,\n        split: str,\n        image_dir: str,\n        df: pandas.DataFrame,\n        image_size=224,\n    ) -> None:\n        self.image_dir = Path(image_dir)\n        X_dict = {\n            "img_fn": df["source_img"].tolist(),\n            "obj_bbox": df["object_bbox"].tolist(),\n            "sub_bbox": df["subject_bbox"].tolist(),\n            "obj_category": df["object_category"].tolist(),\n            "sub_category": df["subject_category"].tolist(),\n        }\n        Y_dict = {\n            "visual_relation_task": torch.LongTensor(df["label"].to_numpy())\n        }  # change to take in the rounded train labels\n        super(SceneGraphDataset, self).__init__(name, split, X_dict, Y_dict)\n\n        # define standard set of transformations to apply to each image\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize((image_size, image_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n\n    def __getitem__(self, index: int) -> Tuple[XDict, YDict]:\n        img_fn = self.X_dict["img_fn"][index]\n        img_arr = np.array(Image.open(self.image_dir / img_fn))\n\n        obj_bbox = self.X_dict["obj_bbox"][index]\n        sub_bbox = self.X_dict["sub_bbox"][index]\n        obj_category = self.X_dict["obj_category"][index]\n        sub_category = self.X_dict["sub_category"][index]\n\n        # compute crops\n        obj_crop = crop_img_arr(img_arr, obj_bbox)\n        sub_crop = crop_img_arr(img_arr, sub_bbox)\n        union_crop = crop_img_arr(img_arr, union(obj_bbox, sub_bbox))\n\n        # transform each crop\n        x_dict = {\n            "obj_crop": self.transform(Image.fromarray(obj_crop)),\n            "sub_crop": self.transform(Image.fromarray(sub_crop)),\n            "union_crop": self.transform(Image.fromarray(union_crop)),\n            "obj_category": obj_category,\n            "sub_category": sub_category,\n        }\n\n        y_dict = {name: label[index] for name, label in self.Y_dict.items()}\n        return x_dict, y_dict\n\n    def __len__(self):\n        return len(self.X_dict["img_fn"])\n\n\nclass WordEmb(nn.Module):\n    """Extract and concat word embeddings for obj and sub categories."""\n\n    def __init__(self, glove_fn="data/glove/glove.6B.100d.txt"):\n        super(WordEmb, self).__init__()\n\n        self.word_embs = pandas.read_csv(\n            glove_fn, sep=" ", index_col=0, header=None, quoting=csv.QUOTE_NONE\n        )\n\n    def _get_wordvec(self, word):\n        return self.word_embs.loc[word].values\n\n    def forward(self, obj_category, sub_category):\n        obj_emb = self._get_wordvec(obj_category)\n        sub_emb = self._get_wordvec(sub_category)\n        embs = np.concatenate([obj_emb, sub_emb], axis=1)\n        return torch.FloatTensor(embs)\n\n\n# Classes and helper functions for defining classifier\ndef init_fc(fc):\n    torch.nn.init.xavier_uniform_(fc.weight)\n    fc.bias.data.fill_(0.01)\n\n\nclass FlatConcat(nn.Module):\n    """Module that flattens and concatenates features"""\n\n    def forward(self, *inputs):\n        return torch.cat([input.view(input.size(0), -1) for input in inputs], dim=1)\n\n\n# Helper functions to geenerate operations\ndef get_op_sequence():\n    # define feature extractors for each of the (union, subject, and object) image crops\n    union_feat_op = Operation(\n        name="union_feat_op",\n        module_name="feat_extractor",\n        inputs=[("_input_", "union_crop")],\n    )\n\n    sub_feat_op = Operation(\n        name="sub_feat_op",\n        module_name="feat_extractor",\n        inputs=[("_input_", "sub_crop")],\n    )\n\n    obj_feat_op = Operation(\n        name="obj_feat_op",\n        module_name="feat_extractor",\n        inputs=[("_input_", "obj_crop")],\n    )\n\n    # define an operation to extract word embeddings for subject and object categories\n    word_emb_op = Operation(\n        name="word_emb_op",\n        module_name="word_emb",\n        inputs=[("_input_", "sub_category"), ("_input_", "obj_category")],\n    )\n\n    # define an operation to concatenate image features and word embeddings\n    concat_op = Operation(\n        name="concat_op",\n        module_name="feat_concat",\n        inputs=["obj_feat_op", "sub_feat_op", "union_feat_op", "word_emb_op"],\n    )\n\n    # define an operation to make a prediction over all concatenated features\n    prediction_op = Operation(\n        name="head_op", module_name="prediction_head", inputs=["concat_op"]\n    )\n\n    return [\n        sub_feat_op,\n        obj_feat_op,\n        union_feat_op,\n        word_emb_op,\n        concat_op,\n        prediction_op,\n    ]\n\n\n# Create model from pre loaded resnet cnn.\ndef create_model(resnet_cnn):\n    # freeze the resnet weights\n    for param in resnet_cnn.parameters():\n        param.requires_grad = False\n\n    # define input features\n    in_features = resnet_cnn.fc.in_features\n    feature_extractor = nn.Sequential(*list(resnet_cnn.children())[:-1])\n\n    # initialize FC layer: maps 3 sets of image features to class logits\n    WEMB_SIZE = 100\n    fc = nn.Linear(in_features * 3 + 2 * WEMB_SIZE, 3)\n    init_fc(fc)\n\n    # define layers\n    module_pool = nn.ModuleDict(\n        {\n            "feat_extractor": feature_extractor,\n            "prediction_head": fc,\n            "feat_concat": FlatConcat(),\n            "word_emb": WordEmb(),\n        }\n    )\n\n    # define task flow through modules\n    op_sequence = get_op_sequence()\n    pred_cls_task = Task(\n        name="visual_relation_task",\n        module_pool=module_pool,\n        op_sequence=op_sequence,\n        scorer=Scorer(metrics=["f1_micro"]),\n    )\n    return MultitaskClassifier([pred_cls_task])\n\n')),(0,r.kt)("h2",{id:"1-\u52a0\u8f09\u6578\u64da\u96c6"},"1. \u52a0\u8f09\u6578\u64da\u96c6"),(0,r.kt)("p",null,"\u4e0b\u8f09 VRD \u6578\u64da\u96c6\u4e26\u904e\u6ffe\u5305\u542b\u81f3\u5c11\u4e00\u500b\u52d5\u4f5c\u8b02\u8a5e\u7684\u5716\u50cf\uff0c\u56e0\u70ba\u9019\u4e9b\u6bd4\u5e7e\u4f55\u95dc\u4fc2\u66f4\u96e3\u5206\u985e\uff0c\u5982above\u6216next to\u3002"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u7bc4\u4f8b\u5c07\u8a13\u7df4\u96c6\u3001\u6709\u6548\u96c6\u548c\u6e2c\u8a66\u96c6\u52a0\u8f09\u70baDataFrame\uff1a",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"label"),": \u5c0d\u8c61\u4e4b\u9593\u7684\u95dc\u4fc2\u3002",(0,r.kt)("inlineCode",{parentName:"li"},"0: RIDE, 1: CARRY, 2:OTHER\u52d5\u4f5c\u8b02\u8a5e")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"object_bbox"),":  ",(0,r.kt)("inlineCode",{parentName:"li"},"[ymin, ymax, xmin, xmax]")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"object_category")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"source_img")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"subject_bbox"),":  ",(0,r.kt)("inlineCode",{parentName:"li"},"[ymin, ymax, xmin, xmax]")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"subject_category")))),(0,r.kt)("li",{parentName:"ul"},"\u6578\u64da\u96c6\u7684\u63a1\u6a23\u7248\u672c\u5728\u8a13\u7df4\u96c6\u3001\u958b\u767c\u96c6\u548c\u6e2c\u8a66\u96c6\u4e0a\u4f7f\u7528\u76f8\u540c\u7684 26 \u500b\u6578\u64da\u3002\u6b64\u8a2d\u7f6e\u65e8\u5728\u5feb\u901f\u6f14\u793a Snorkel \u5982\u4f55\u8655\u7406\u6b64\u4efb\u52d9\uff0c\u800c\u4e0d\u662f\u6f14\u793a\u6027\u80fd\u3002")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import os\nfrom utils import load_vrd_data\n\n# setting sample=False will take ~3 hours to run (downloads full VRD dataset)\nsample = True\nis_test = os.environ.get("TRAVIS") == "true" or os.environ.get("IS_TEST") == "true"\ndf_train, df_valid, df_test = load_vrd_data(sample, is_test)\n\nprint("Train Relationships: ", len(df_train))\nprint("Dev Relationships: ", len(df_valid))\nprint("Test Relationships: ", len(df_test))\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"df_train.head()\n")),(0,r.kt)("p",null,"\u8acb\u6ce8\u610f\uff0c\u8a13\u7df4DataFrame\u5c07\u6709\u4e00\u500b\u5168\u70ba -1 \u7684\u6a19\u7c64\u5b57\u6bb5\u3002\u9019\u8868\u793a\u8a72\u7279\u5b9a\u6578\u64da\u96c6\u7f3a\u5c11\u6a19\u7c64\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u5011\u5c07\u901a\u904e\u5728\u4e3b\u9ad4\u548c\u5ba2\u9ad4\u7684\u5c6c\u6027\u4e0a\u7de8\u5beb\u6a19\u7c64\u51fd\u6578\u4f86\u70ba\u8a13\u7df4\u96c6\u5206\u914d\u6982\u7387\u6a19\u7c64\uff01"),(0,r.kt)("h2",{id:"2-\u7de8\u5beblabeling-functions-lfs"},"2. \u7de8\u5bebLabeling Functions (LFs)"),(0,r.kt)("p",null,"\u6211\u5011\u73fe\u5728\u7de8\u5beb\u6a19\u8a18\u51fd\u6578\u4f86\u6aa2\u6e2c\u908a\u754c\u6846\u5c0d\u4e4b\u9593\u5b58\u5728\u4ec0\u9ebc\u95dc\u4fc2\u3002\u70ba\u6b64\uff0c\u6211\u5011\u53ef\u4ee5\u5c07\u5404\u7a2e\u76f4\u89ba\u7de8\u78bc\u5230\u6a19\u8a18\u51fd\u6578\u4e2d\uff1a"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u5206\u985e\u76f4\u89ba\uff1a\u95dc\u65bc\u9019\u4e9b\u95dc\u4fc2\u4e2d\u901a\u5e38\u6d89\u53ca\u7684\u4e3b\u8a5e\u8207\u53d7\u8a5e\u985e\u5225\u7684\u77e5\u8b58\uff08\u4f8b\u5982\uff0cperson\u901a\u5e38\u662f\u52d5\u8a5eride\u548c\u7684\u4e3b\u8a5ecarry\uff09"),(0,r.kt)("li",{parentName:"ul"},"\u7a7a\u9593\u76f4\u89ba\uff1a\u95dc\u65bc\u4e3b\u8a5e\u8207\u52d5\u8a5e\u7684\u76f8\u5c0d\u4f4d\u7f6e\u7684\u77e5\u8b58\uff08\u4f8b\u5982\uff0c\u4e3b\u8a5e\u901a\u5e38\u9ad8\u65bc\u52d5\u8a5e\u7684\u53d7\u8a5eride\uff09")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"RIDE = 0\nCARRY = 1\nOTHER = 2\nABSTAIN = -1\n")),(0,r.kt)("p",null,"\u6211\u5011\u5f9e\u7de8\u78bc\u5206\u985e\u76f4\u89ba\u7684\u6a19\u8a18\u51fd\u6578\u958b\u59cb\uff1a\u6211\u5011\u4f7f\u7528\u95dc\u65bc\u5171\u540c\u7684\u4e3b\u984c-\u5ba2\u9ad4\u985e\u5225\u5c0d\u7684\u77e5\u8b58RIDE\uff0cCARRY\u4ee5\u53ca\u95dc\u65bc\u54ea\u4e9b\u4e3b\u984c\u6216\u5ba2\u9ad4\u4e0d\u592a\u53ef\u80fd\u6d89\u53ca\u9019\u5169\u7a2e\u95dc\u4fc2\u7684\u77e5\u8b58\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from snorkel.labeling import labeling_function\n\n# Category-based LFs\n@labeling_function()\ndef lf_ride_object(x):\n    if x.subject_category == "person":\n        if x.object_category in [\n            "bike",\n            "snowboard",\n            "motorcycle",\n            "horse",\n            "bus",\n            "truck",\n            "elephant",\n        ]:\n            return RIDE\n    return ABSTAIN\n\n\n@labeling_function()\ndef lf_carry_object(x):\n    if x.subject_category == "person":\n        if x.object_category in ["bag", "surfboard", "skis"]:\n            return CARRY\n    return ABSTAIN\n\n\n@labeling_function()\ndef lf_carry_subject(x):\n    if x.object_category == "person":\n        if x.subject_category in ["chair", "bike", "snowboard", "motorcycle", "horse"]:\n            return CARRY\n    return ABSTAIN\n\n\n@labeling_function()\ndef lf_not_person(x):\n    if x.subject_category != "person":\n        return OTHER\n    return ABSTAIN\n')),(0,r.kt)("p",null,"\u73fe\u5728\u7de8\u78bc\u7a7a\u9593\u76f4\u89ba\uff0c\u5176\u4e2d\u5305\u62ec\u6e2c\u91cf\u908a\u754c\u6846\u4e4b\u9593\u7684\u8ddd\u96e2\u4e26\u6bd4\u8f03\u5b83\u5011\u7684\u76f8\u5c0d\u5340\u57df\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"YMIN = 0\nYMAX = 1\nXMIN = 2\nXMAX = 3\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\n\n# Distance-based LFs\n@labeling_function()\ndef lf_ydist(x):\n    if x.subject_bbox[XMAX] < x.object_bbox[XMAX]:\n        return OTHER\n    return ABSTAIN\n\n\n@labeling_function()\ndef lf_dist(x):\n    if np.linalg.norm(np.array(x.subject_bbox) - np.array(x.object_bbox)) <= 1000:\n        return OTHER\n    return ABSTAIN\n\n\ndef area(bbox):\n    return (bbox[YMAX] - bbox[YMIN]) * (bbox[XMAX] - bbox[XMIN])\n\n\n# Size-based LF\n@labeling_function()\ndef lf_area(x):\n    if area(x.subject_bbox) / area(x.object_bbox) <= 0.5:\n        return OTHER\n    return ABSTAIN\n")),(0,r.kt)("p",null,"\u6a19\u8a18\u51fd\u6578\u5177\u6709\u4e0d\u540c\u7684\u7d93\u9a57\u6e96\u78ba\u6027\u548c\u8986\u84cb\u7bc4\u570d\u3002\u7531\u65bc\u6211\u5011\u9078\u64c7\u7684\u95dc\u4fc2\u4e2d\u7684\u985e\u5225\u4e0d\u5e73\u8861\uff0c\u6a19\u8a18OTHER\u985e\u7684\u6a19\u8a18\u51fd\u6578\u6bd4RIDE\u6216CARRY\u7684\u6a19\u8a18\u51fd\u6578\u5177\u6709\u66f4\u9ad8\u7684\u8986\u84cb\u7387\u3002\u9019\u4e5f\u53cd\u6620\u4e86\u6578\u64da\u96c6\u4e2d\u985e\u7684\u5206\u4f48\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'tags=["md-exclude-output"]',tags:'["md-exclude-output"]'},"from snorkel.labeling import PandasLFApplier\n\nlfs = [\n    lf_ride_object,\n    lf_carry_object,\n    lf_carry_subject,\n    lf_not_person,\n    lf_ydist,\n    lf_dist,\n    lf_area,\n]\n\napplier = PandasLFApplier(lfs)\nL_train = applier.apply(df_train)\nL_valid = applier.apply(df_valid)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from snorkel.labeling import LFAnalysis\n\nY_valid = df_valid.label.values\nLFAnalysis(L_valid, lfs).lf_summary(Y_valid)\n")),(0,r.kt)("h2",{id:"3-\u8a13\u7df4\u6a19\u7c64\u6a21\u578b"},"3. \u8a13\u7df4\u6a19\u7c64\u6a21\u578b"),(0,r.kt)("p",null,"\u8a13\u7df4",(0,r.kt)("inlineCode",{parentName:"p"},"LabelModel"),"\u4f86\u70ba\u672a\u6a19\u8a18\u7684\u8a13\u7df4\u96c6\u5206\u914d\u8a13\u7df4\u6a19\u7c64\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from snorkel.labeling.model import LabelModel\n\nlabel_model = LabelModel(cardinality=3, verbose=True)\nlabel_model.fit(\n    L_train, \n    seed=123, \n    lr=0.01, \n    log_freq=10, \n    n_epochs=100\n    )\n")),(0,r.kt)("p",null,"\u4f7f\u7528F1\u8861\u91cf\u6a21\u578b\n",(0,r.kt)("inlineCode",{parentName:"p"},"F1 = 2 * (precision * recall) / (precision + recall)")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'label_model.score(L_valid, Y_valid, metrics=["f1_micro"])\n')),(0,r.kt)("h2",{id:"4-\u8a13\u7df4\u5206\u985e\u5668"},"4. \u8a13\u7df4\u5206\u985e\u5668"),(0,r.kt)("p",null,"\u73fe\u5728\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u9019\u4e9b\u8a13\u7df4\u6a19\u7c64\u4f86\u8a13\u7df4\u4efb\u4f55\u6a19\u6e96\u5224\u5225\u6a21\u578b\uff0c\u4f8b\u5982\u73fe\u6210\u7684 ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/KaimingHe/deep-residual-networks"},"ResNet"),"\uff0c\u5b83\u61c9\u8a72\u5b78\u6703\u5728\u6211\u5011\u958b\u767c\u7684 LF \u4e4b\u5916\u9032\u884c\u6cdb\u5316\uff01"),(0,r.kt)("h4",{id:"create-dataloaders-for-classifier"},"Create DataLoaders for Classifier"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from snorkel.classification import DictDataLoader\nfrom model import SceneGraphDataset, create_model\n\ndf_train["labels"] = label_model.predict(L_train)\n\nif sample:\n    TRAIN_DIR = "data/VRD/sg_dataset/samples"\nelse:\n    TRAIN_DIR = "data/VRD/sg_dataset/sg_train_images"\n\ndl_train = DictDataLoader(\n    SceneGraphDataset("train_dataset", "train", TRAIN_DIR, df_train),\n    batch_size=16,\n    shuffle=True,\n)\n\ndl_valid = DictDataLoader(\n    SceneGraphDataset("valid_dataset", "valid", TRAIN_DIR, df_valid),\n    batch_size=16,\n    shuffle=False,\n)\n')),(0,r.kt)("h4",{id:"\u5b9a\u7fa9\u6a21\u578b\u67b6\u69cb"},"\u5b9a\u7fa9\u6a21\u578b\u67b6\u69cb"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import torchvision.models as models\n\n# initialize pretrained feature extractor\ncnn = models.resnet18(pretrained=True)\nmodel = create_model(cnn)\n")),(0,r.kt)("h3",{id:"\u8a13\u7df4\u8207\u8a55\u4f30\u6a21\u578b"},"\u8a13\u7df4\u8207\u8a55\u4f30\u6a21\u578b"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'tags=["md-exclude-output"]',tags:'["md-exclude-output"]'},'from snorkel.classification import Trainer\n\ntrainer = Trainer(\n    n_epochs=1,  # increase for improved performance\n    lr=1e-3,\n    checkpointing=True,\n    checkpointer_config={"checkpoint_dir": "checkpoint"},\n)\ntrainer.fit(model, [dl_train])\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"model.score([dl_valid])\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6211\u5011\u5df2\u7d93\u6210\u529f\u8a13\u7df4\u4e86\u4e00\u500b\u8996\u89ba\u95dc\u4fc2\u6aa2\u6e2c\u6a21\u578b\uff01\u4f7f\u7528\u95dc\u65bc\u8996\u89ba\u95dc\u4fc2\u4e2d\u7684\u5c0d\u50cf\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u7684\u5206\u985e\u548c\u7a7a\u9593\u76f4\u89ba\uff0c\u6211\u5011\u80fd\u5920\u5728\u591a\u985e\u5206\u985e\u8a2d\u7f6e\u4e2d\u70ba VRD \u6578\u64da\u96c6\u4e2d\u7684\u5c0d\u50cf\u5c0d\u5206\u914d\u9ad8\u8cea\u91cf\u7684\u8a13\u7df4\u6a19\u7c64\u3002")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\u6709\u95dc Snorkel \u5982\u4f55\u7528\u65bc\u8996\u89ba\u95dc\u4fc2\u4efb\u52d9\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8acb\u53c3\u95b1",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1904.11622"},"ICCV 2019 \u8ad6\u6587"),"\uff01"))))}m.isMDXComponent=!0}}]);